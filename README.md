# note

this is just a fork of the awesome ullm code base, just so I can play with it and saving it for later.
the ultimate goal is to make this work on an Atari ST (probably an emulated falcon with some DSP56k code for the matmul if I manage to find a mac cross compiler that'll produce working dsp code) :)
All the matmul variants I've added have been generated by ChatGPT 4o. 

------------------
# ullm

An effort to bring LLMs to very resource-constrained systems.

This is a toy project. Please don't take it too seriously :)

## Build, Test and Run

### Bazel

Bazel is the primary and modern way to build and test.

Build:

```
bazel build tools:ullm
```

Test:

```
bazel test ...
```

Run:

```
bazel run tools/ullm -- -p "The quick brown fox jumped. Where did they go?"
```

### Makefile

A Makefile is offered as a way to more easily tweak the build. It is not as
full-featured as Bazel.

Build:

```
make clean && make fetchdeps && make -j`nproc`
```

Run:

```
./out/ullm.elf -c out/stories15M.bin -t out/llama2.c/tokenizer.bin -p "The quick brown fox jumped. Where did he go?"
```

## Ports

### llama2.c

`ullm` contains a heavily modified version of the `llama2.c` project.

Forked from [llama2.c](https://github.com/karpathy/llama2.c/tree/350e04fe35433e6d2941dce5a1f53308f87058eb).

This code retains the MIT license, and the headers indicate as such.

## Credits

Thanks to Andrej Karpathy for the llama2.c project. I really enjoyed
experimenting with the code.
